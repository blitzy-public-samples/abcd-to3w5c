name: Habit Service CI/CD

on:
  push:
    branches: [ main ]
    paths:
      - 'src/backend/src/habit-service/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/backend/src/habit-service/**'

# Concurrency control to prevent parallel deployments
concurrency:
  group: habit-service-${{ github.ref }}
  cancel-in-progress: true

# Security permissions
permissions:
  contents: read
  packages: write
  id-token: write
  security-events: write

env:
  AWS_REGION: us-west-2
  ECR_REPOSITORY: habit-tracker/habit-service
  NODE_VERSION: '18.x'

jobs:
  test:
    name: Test and Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: src/backend/src/habit-service/package-lock.json

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.npm
          key: npm-${{ hashFiles('src/backend/src/habit-service/package-lock.json') }}
          restore-keys: npm-

      - name: Install dependencies
        working-directory: src/backend/src/habit-service
        run: |
          npm ci
          npm audit

      - name: Run linting and static analysis
        working-directory: src/backend/src/habit-service
        run: |
          npm run lint
          npx tsc --noEmit

      - name: Run unit tests with coverage
        working-directory: src/backend/src/habit-service
        run: npm run test
        env:
          CI: true

      - name: Run integration tests
        working-directory: src/backend/src/habit-service
        run: npm run test:e2e
        env:
          CI: true

      - name: Upload coverage reports
        uses: actions/upload-artifact@v3
        with:
          name: coverage-reports
          path: src/backend/src/habit-service/coverage
          retention-days: 14

  build-and-push:
    name: Build and Push Container
    needs: test
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1
        id: ecr-login

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Cache Docker layers
        uses: actions/cache@v3
        with:
          path: /tmp/.buildx-cache
          key: buildx-${{ github.sha }}
          restore-keys: buildx-

      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: src/backend/src/habit-service
          file: src/backend/src/habit-service/Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ steps.ecr-login.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}
            ${{ steps.ecr-login.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new
          build-args: |
            NODE_ENV=production
            VERSION=${{ github.sha }}

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@v0.10.0
        with:
          image-ref: ${{ steps.ecr-login.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '1'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  deploy:
    name: Deploy to Production
    needs: build-and-push
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: 
      name: production
      url: https://api.habit-tracker.com/habits
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name habit-tracker-cluster --region ${{ env.AWS_REGION }}
        env:
          KUBECONFIG: ${{ secrets.KUBE_CONFIG }}

      - name: Deploy to Kubernetes
        run: |
          # Update image tag in deployment
          kubectl set image deployment/habit-service \
            habit-service=${{ steps.ecr-login.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }} \
            -n default

          # Wait for rollout
          kubectl rollout status deployment/habit-service -n default --timeout=300s

      - name: Verify deployment
        run: |
          # Check deployment status
          READY=$(kubectl get deployment habit-service -n default -o jsonpath='{.status.readyReplicas}')
          DESIRED=$(kubectl get deployment habit-service -n default -o jsonpath='{.spec.replicas}')
          
          if [ "$READY" != "$DESIRED" ]; then
            echo "Deployment verification failed: Ready pods ($READY) != Desired pods ($DESIRED)"
            exit 1
          fi

          # Verify health endpoints
          kubectl wait --for=condition=ready pod -l app=habit-tracker,service=habit-service -n default --timeout=300s
          
          # Check application health
          for i in {1..3}; do
            HEALTH_STATUS=$(kubectl exec -n default \
              $(kubectl get pod -l app=habit-tracker,service=habit-service -n default -o jsonpath='{.items[0].metadata.name}') \
              -- curl -s http://localhost:3002/health)
            
            if [ "$HEALTH_STATUS" != '{"status":"ok"}' ]; then
              echo "Health check failed"
              exit 1
            fi
          done

      - name: Configure monitoring
        run: |
          # Apply Prometheus ServiceMonitor
          kubectl apply -f - <<EOF
          apiVersion: monitoring.coreos.com/v1
          kind: ServiceMonitor
          metadata:
            name: habit-service-monitor
            namespace: monitoring
          spec:
            selector:
              matchLabels:
                app: habit-tracker
                service: habit-service
            endpoints:
            - port: metrics
              interval: 15s
          EOF

      - name: Update deployment status
        if: always()
        uses: actions/github-script@v6
        with:
          script: |
            const { owner, repo } = context.repo;
            const deployment = await github.rest.repos.createDeployment({
              owner,
              repo,
              ref: context.sha,
              environment: 'production',
              auto_merge: false,
              required_contexts: [],
              description: 'Production deployment of Habit Service'
            });
            
            await github.rest.repos.createDeploymentStatus({
              owner,
              repo,
              deployment_id: deployment.data.id,
              state: '${{ job.status }}',
              environment_url: 'https://api.habit-tracker.com/habits',
              description: 'Deployment ${{ job.status }}'
            });