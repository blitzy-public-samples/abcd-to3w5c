# Logstash Configuration v8.x
# Purpose: Log aggregation and processing for habit tracking application
# Dependencies: Elasticsearch v8.x, Filebeat v8.x, Metricbeat v8.x

# Global Settings
# These settings affect all pipeline components
pipeline.workers: ${PIPELINE_WORKERS:2}
pipeline.batch.size: ${BATCH_SIZE:125}
pipeline.batch.delay: ${BATCH_DELAY:50}
queue.type: persisted
queue.max_bytes: 1gb
path.data: /var/lib/logstash
log.level: ${LOG_LEVEL:info}

# Input Section
# Defines all log input sources with secure configurations
input {
  # Filebeat input for application logs
  beats {
    port => 5044
    type => "beats"
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify_mode => "peer"
    include_codec_tag => true
  }

  # TCP input for direct application logging
  tcp {
    port => 5000
    type => "tcp"
    codec => json_lines
    ssl_enable => true
    ssl_cert => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify => true
  }

  # HTTP input for REST-based log ingestion
  http {
    port => 8080
    type => "http"
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    verify_mode => "peer"
    response_headers => {
      "Content-Type" => "application/json"
      "Access-Control-Allow-Origin" => "https://api.habit-tracker.com"
    }
  }
}

# Filter Section
# Advanced log processing and enrichment rules
filter {
  # Common timestamp processing
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
    timezone => "UTC"
  }

  # Application log parsing
  if [type] == "application" {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:service}\] \[%{DATA:trace_id}\] %{GREEDYDATA:message}",
          "%{TIMESTAMP_ISO8601:timestamp} %{WORD:level} %{IP:client_ip} %{WORD:http_method} %{URIPATHPARAM:request_path} %{NUMBER:response_code}"
        ]
      }
      tag_on_failure => ["_grokparsefailure", "application_parse_failure"]
    }
  }

  # Security event parsing
  if [type] == "security" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} SECURITY \[%{DATA:security_level}\] \[%{DATA:security_component}\] %{GREEDYDATA:security_message}"
      }
      tag_on_failure => ["_grokparsefailure", "security_parse_failure"]
    }
    
    # Add security severity classification
    ruby {
      code => '
        security_level = event.get("security_level")
        event.set("severity", 
          case security_level
          when "CRITICAL" then 1
          when "HIGH" then 2
          when "MEDIUM" then 3
          when "LOW" then 4
          else 5
          end
        )
      '
    }
  }

  # Enrich all events with metadata
  mutate {
    add_field => {
      "environment" => "${ENV:production}"
      "application" => "habit-tracker"
      "version" => "${APP_VERSION}"
      "datacenter" => "${DC_LOCATION}"
      "host_ip" => "%{[host][ip]}"
    }
    convert => {
      "response_code" => "integer"
      "response_time" => "float"
    }
  }

  # Add processing timestamp
  ruby {
    code => "event.set('processing_timestamp', Time.now.utc)"
  }

  # Drop sensitive fields
  mutate {
    remove_field => ["password", "secret", "token", "api_key"]
  }
}

# Output Section
# Configure secure log output destinations
output {
  # Primary Elasticsearch output
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "habit-tracker-%{+YYYY.MM.dd}"
    template_name => "habit-tracker"
    template_overwrite => true
    
    # Index Lifecycle Management
    ilm_enabled => true
    ilm_rollover_alias => "habit-tracker"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "habit-tracker-policy"
    
    # Security settings
    ssl => true
    ssl_certificate_verification => true
    user => "${ELASTIC_USER}"
    password => "${ELASTIC_PASSWORD}"
    
    # Retry configuration
    retry_on_conflict => 3
    action => "index"
    
    # Buffer settings
    flush_size => 500
    idle_flush_time => 1
  }

  # Dead Letter Queue for failed events
  if "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "logstash-dlq-%{+YYYY.MM}"
      user => "${ELASTIC_USER}"
      password => "${ELASTIC_PASSWORD}"
      ssl => true
    }
  }

  # Debug output (disabled in production)
  if [log_level] == "debug" {
    stdout {
      codec => rubydebug
    }
  }
}